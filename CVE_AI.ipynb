{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmDRsTQbtkHWfnH3tc8nzH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1948023/AI_Risk_Tool/blob/main/CVE_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXiBXNnDhzqB",
        "outputId": "4b47ade1-0d92-4198-9040-a4afd47d4a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import json\n",
        "import os\n",
        "from xml.etree import ElementTree as ET\n",
        "\n",
        "# üîß CONFIG\n",
        "CPE_FILE = \"official-cpe-dictionary_v2.3.xml.gz\"\n",
        "CVE_FILES = [\n",
        "    \"nvdcve-2.0-2025.json.gz\",\n",
        "    \"nvdcve-2.0-2024.json.gz\",\n",
        "    \"nvdcve-2.0-2023.json.gz\",\n",
        "    \"nvdcve-2.0-2022.json.gz\"\n",
        "]\n",
        "KEYWORDS = [\"vxworks\", \"qnx\", \"rtems\", \"integrity\", \"nucleus\", \"threadx\", \"micrium\",\n",
        "    \"freeRTOS\", \"zephyr\", \"ti-rtos\", \"embos\", \"ucos\", \"satellite-toolkit\",\n",
        "    \"cosmos\", \"coreflightexec\", \"coreflight\", \"flightsoftware\", \"gnuradio\",\n",
        "    \"uhd\", \"hackrf\", \"bladeRF\", \"ettus\", \"openbts\", \"srsLTE\", \"srsRAN\", \"ccsds\",\n",
        "    \"libccsds\", \"dvb-s\", \"dvb-s2\", \"modcod\", \"aes\", \"ecc\", \"spacewire\", \"ssh\",\n",
        "    \"tls\", \"http\", \"snmp\", \"ntp\", \"ftp\", \"l3harris\", \"thales\", \"airbus\", \"boeing\",\n",
        "    \"cobham\", \"raytheon\", \"ball\", \"northrop\", \"sierra\", \"ohb\", \"maxar\", \"viasat\",\n",
        "    \"spacex\", \"blueorigin\", \"rocketlab\", \"openssh\", \"openssl\", \"apache\", \"nginx\",\n",
        "    \"postgres\", \"mysql\", \"ubuntu\", \"debian\", \"windows_server\", \"grafana\",\n",
        "    \"kibana\", \"elasticsearch\", \"zabbix\", \"nagios\", \"docker\", \"kubernetes\",\n",
        "    \"satellite-control\", \"groundstation\", \"egse\", \"telemetry\", \"tm-tc\"]\n",
        "\n",
        "OUTPUT_FILE = \"output_cve.txt\"\n",
        "\n",
        "def load_json_gz(filename):\n",
        "    try:\n",
        "        with gzip.open(filename, 'rt', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Errore nel caricamento {filename}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_cpe_dictionary(filename):\n",
        "    cpes = set()\n",
        "    try:\n",
        "        with gzip.open(filename, 'rb') as f:\n",
        "            tree = ET.parse(f)\n",
        "            root = tree.getroot()\n",
        "            for item in root.findall('{http://cpe.mitre.org/dictionary/2.0}cpe-item'):\n",
        "                name = item.get('name')\n",
        "                if name and any(keyword.lower() in name.lower() for keyword in KEYWORDS):\n",
        "                    cpes.add(name)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Errore nel parsing CPE: {e}\")\n",
        "    return cpes\n",
        "\n",
        "def extract_cpes_from_nodes(nodes):\n",
        "    cpes = set()\n",
        "    for node in nodes:\n",
        "        matches = node.get(\"cpe_match\", [])\n",
        "        for match in matches:\n",
        "            if match.get(\"vulnerable\", False):\n",
        "                cpe_uri = match.get(\"cpe23Uri\", \"\")\n",
        "                if cpe_uri:\n",
        "                    cpes.add(cpe_uri)\n",
        "        # Ricorsione per child nodes\n",
        "        children = node.get(\"children\", [])\n",
        "        if children:\n",
        "            cpes.update(extract_cpes_from_nodes(children))\n",
        "    return cpes\n",
        "\n",
        "def find_matching_cves(cve_data, cpe_set):\n",
        "    found_cves = []\n",
        "    if not cve_data:\n",
        "        return found_cves\n",
        "\n",
        "    for item in cve_data.get(\"CVE_Items\", []):\n",
        "        cve_id = item.get(\"cve\", {}).get(\"CVE_data_meta\", {}).get(\"ID\", \"\")\n",
        "        nodes = item.get(\"configurations\", {}).get(\"nodes\", [])\n",
        "        cpes_found = extract_cpes_from_nodes(nodes)\n",
        "\n",
        "        for cpe_uri in cpes_found:\n",
        "            if cpe_uri in cpe_set:\n",
        "                # CVSS v3.1 > v3.0 > v2\n",
        "                score = \"-\"\n",
        "                metrics = item.get(\"impact\", {})\n",
        "                for key in [\"baseMetricV3\", \"baseMetricV2\"]:\n",
        "                    if key in metrics:\n",
        "                        score = metrics[key].get(\"cvssV3\", {}).get(\"baseScore\") or metrics[key].get(\"cvssV2\", {}).get(\"baseScore\") or \"-\"\n",
        "                        break\n",
        "                found_cves.append((cve_id, cpe_uri, score))\n",
        "    return found_cves\n",
        "\n",
        "def main():\n",
        "    print(\"üì¶ Caricamento dizionario CPE...\")\n",
        "    cpe_set = load_cpe_dictionary(CPE_FILE)\n",
        "    print(f\"‚úÖ Trovati {len(cpe_set)} CPE rilevanti.\")\n",
        "\n",
        "    all_cves = []\n",
        "    for cve_file in CVE_FILES:\n",
        "        print(f\"\\nüîç Analisi file CVE: {cve_file}\")\n",
        "        cve_data = load_json_gz(cve_file)\n",
        "        cves = find_matching_cves(cve_data, cpe_set)\n",
        "        print(f\"‚úÖ Trovate {len(cves)} CVE nel file {cve_file}\")\n",
        "        all_cves.extend(cves)\n",
        "\n",
        "    with open(OUTPUT_FILE, \"w\", encoding='utf-8') as f:\n",
        "        for cve_id, cpe_uri, score in all_cves:\n",
        "            f.write(f\"{cve_id} | {cpe_uri} | CVSS: {score}\\n\")\n",
        "\n",
        "    print(f\"\\nüìÅ Output salvato in: {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkJEm2i0F1ko",
        "outputId": "97bfac3d-e0bf-4a1f-a5ab-db293b3a96d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Caricamento CPE...\n",
            "‚úÖ Trovati 59930 CPE rilevanti.\n",
            "\n",
            "üîç Analisi file CVE: nvdcve-2.0-2025.json.gz\n",
            "‚úÖ Trovate 0 CVE nel file nvdcve-2.0-2025.json.gz\n",
            "\n",
            "üîç Analisi file CVE: nvdcve-2.0-2024.json.gz\n",
            "‚úÖ Trovate 0 CVE nel file nvdcve-2.0-2024.json.gz\n",
            "\n",
            "üîç Analisi file CVE: nvdcve-2.0-2023.json.gz\n",
            "‚úÖ Trovate 0 CVE nel file nvdcve-2.0-2023.json.gz\n",
            "\n",
            "üîç Analisi file CVE: nvdcve-2.0-2022.json.gz\n",
            "‚úÖ Trovate 0 CVE nel file nvdcve-2.0-2022.json.gz\n",
            "\n",
            "üìÅ Output salvato in: output_cve.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "# üîß Config\n",
        "EPSS_CSV_URL = \"https://epss.cyentia.com/epss_scores-current.csv.gz\"\n",
        "LOCAL_EPSS_FILE = \"/content/epss_scores-current.csv.gz\"\n",
        "INPUT_FILE = \"/content/output_cve.txt\"\n",
        "OUTPUT_FILE = \"/content/cve_with_epss.csv\"\n",
        "INTERMEDIATE_DIR = \"/content/intermediate_files\"\n",
        "\n",
        "def get_current_time():\n",
        "    \"\"\"Restituisce la data e ora corrente in formato UTC\"\"\"\n",
        "    return datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def ensure_directory(directory):\n",
        "    \"\"\"Crea una directory se non esiste\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        print(f\"[{get_current_time()}] üìÅ Creata directory: {directory}\")\n",
        "\n",
        "def download_epss_file():\n",
        "    \"\"\"Scarica il database EPSS\"\"\"\n",
        "    print(f\"[{get_current_time()}] ‚¨áÔ∏è Scaricando EPSS database...\")\n",
        "    r = requests.get(EPSS_CSV_URL)\n",
        "    if r.status_code == 200:\n",
        "        ensure_directory(INTERMEDIATE_DIR)\n",
        "        with open(LOCAL_EPSS_FILE, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "        print(f\"[{get_current_time()}] ‚úÖ EPSS database scaricato.\")\n",
        "\n",
        "        # Salva una copia nel directory intermedio\n",
        "        intermediate_epss = os.path.join(INTERMEDIATE_DIR, \"epss_database.csv.gz\")\n",
        "        with open(intermediate_epss, \"wb\") as f:\n",
        "            f.write(r.content)\n",
        "    else:\n",
        "        raise Exception(f\"Errore download EPSS database: {r.status_code}\")\n",
        "\n",
        "def load_cve_from_file():\n",
        "    \"\"\"Carica e analizza il file output_cve.txt\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(INPUT_FILE):\n",
        "            print(f\"[{get_current_time()}] ‚ö†Ô∏è File output_cve.txt non trovato.\")\n",
        "            print(\"Carica manualmente il file output_cve.txt...\")\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                raise Exception(\"Nessun file caricato!\")\n",
        "            with open(INPUT_FILE, 'wb') as f:\n",
        "                f.write(next(iter(uploaded.values())))\n",
        "    except Exception as e:\n",
        "        print(f\"[{get_current_time()}] ‚ùå Errore durante il caricamento del file: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    print(f\"[{get_current_time()}] üìñ Leggendo CVE dal file...\")\n",
        "    cve_ids = set()\n",
        "\n",
        "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    cve_pattern = r'CVE-\\d{4}-\\d+|CVE-\\d+-\\d+'\n",
        "    matches = re.findall(cve_pattern, content, re.IGNORECASE)\n",
        "    cve_ids = {cve.upper() for cve in matches}\n",
        "\n",
        "    # Salva i CVE trovati in un file intermedio\n",
        "    ensure_directory(INTERMEDIATE_DIR)\n",
        "    intermediate_cve = os.path.join(INTERMEDIATE_DIR, \"extracted_cves.txt\")\n",
        "    with open(intermediate_cve, 'w', encoding='utf-8') as f:\n",
        "        for cve in sorted(cve_ids):\n",
        "            f.write(f\"{cve}\\n\")\n",
        "\n",
        "    print(f\"[{get_current_time()}] üîç Trovati {len(cve_ids)} CVE unici.\")\n",
        "    return list(cve_ids)\n",
        "\n",
        "def clean_epss_data(df):\n",
        "    \"\"\"Pulisce e formatta i dati EPSS in modo pi√π robusto\"\"\"\n",
        "    try:\n",
        "        # Resetta l'indice per ottenere i CVE come colonna\n",
        "        if df.index.name is None and any('CVE-' in str(idx) for idx in df.index):\n",
        "            df = df.reset_index()\n",
        "\n",
        "        # Identifica la colonna CVE\n",
        "        cve_col = None\n",
        "        for col in df.columns:\n",
        "            if any('CVE-' in str(val) for val in df[col].head()):\n",
        "                cve_col = col\n",
        "                break\n",
        "\n",
        "        if cve_col is None:\n",
        "            raise ValueError(\"Nessuna colonna CVE trovata\")\n",
        "\n",
        "        # Identifica la colonna EPSS\n",
        "        epss_col = None\n",
        "        for col in df.columns:\n",
        "            try:\n",
        "                # Prova a convertire i valori in float\n",
        "                test_vals = pd.to_numeric(df[col].str.split().str[0], errors='coerce')\n",
        "                if test_vals.notna().any() and test_vals.max() <= 1.0:\n",
        "                    epss_col = col\n",
        "                    break\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if epss_col is None:\n",
        "            raise ValueError(\"Nessuna colonna EPSS valida trovata\")\n",
        "\n",
        "        # Crea nuovo DataFrame pulito\n",
        "        clean_df = pd.DataFrame()\n",
        "        clean_df['cve'] = df[cve_col].astype(str).str.upper()\n",
        "        clean_df['epss'] = pd.to_numeric(df[epss_col].str.split().str[0], errors='coerce')\n",
        "\n",
        "        # Pulizia finale\n",
        "        clean_df = clean_df[clean_df['cve'].str.contains('CVE-', na=False)]\n",
        "        clean_df = clean_df.dropna(subset=['epss'])\n",
        "\n",
        "        # Rimuovi eventuali duplicati\n",
        "        clean_df = clean_df.drop_duplicates(subset=['cve'])\n",
        "\n",
        "        return clean_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Errore durante la pulizia dei dati: {str(e)}\")\n",
        "        print(\"\\nStruttura del DataFrame originale:\")\n",
        "        print(df.head())\n",
        "        print(\"\\nColonne disponibili:\", df.columns.tolist())\n",
        "        print(\"\\nTipi di dati:\")\n",
        "        print(df.dtypes)\n",
        "        raise\n",
        "\n",
        "def match_epss(cve_ids):\n",
        "    \"\"\"Matcha i CVE con i loro EPSS scores\"\"\"\n",
        "    print(f\"[{get_current_time()}] üìä Caricamento database EPSS...\")\n",
        "\n",
        "    try:\n",
        "        # Carica il database EPSS\n",
        "        epss_df = pd.read_csv(LOCAL_EPSS_FILE, compression='gzip', low_memory=False)\n",
        "        print(\"\\nDataFrame originale:\")\n",
        "        print(epss_df.head())\n",
        "        print(\"\\nTipi di dati:\")\n",
        "        print(epss_df.dtypes)\n",
        "\n",
        "        # Pulisci i dati\n",
        "        epss_df = clean_epss_data(epss_df)\n",
        "\n",
        "        print(\"\\nDataFrame dopo pulizia:\")\n",
        "        print(epss_df.head())\n",
        "\n",
        "        # Match e tracking delle statistiche\n",
        "        matched = []\n",
        "        found = 0\n",
        "        not_found = 0\n",
        "        not_found_examples = []\n",
        "\n",
        "        for cve_id in cve_ids:\n",
        "            epss_score = None\n",
        "            # Cerca il CVE nel database\n",
        "            matching_rows = epss_df[epss_df['cve'] == cve_id]\n",
        "            if not matching_rows.empty:\n",
        "                epss_score = matching_rows['epss'].iloc[0]\n",
        "                found += 1\n",
        "            else:\n",
        "                not_found += 1\n",
        "                if len(not_found_examples) < 5:\n",
        "                    not_found_examples.append(cve_id)\n",
        "            matched.append((cve_id, epss_score))\n",
        "\n",
        "        # Stampa statistiche\n",
        "        print(f\"\\n[{get_current_time()}] üìà Statistiche matching:\")\n",
        "        print(f\"   ‚úÖ CVE trovati: {found}\")\n",
        "        print(f\"   ‚ùå CVE non trovati: {not_found}\")\n",
        "        if not_found_examples:\n",
        "            print(f\"   üìù Esempi di CVE non trovati: {', '.join(not_found_examples)}\")\n",
        "\n",
        "        # Salva debug info\n",
        "        ensure_directory(INTERMEDIATE_DIR)\n",
        "        debug_info = {\n",
        "            'timestamp': get_current_time(),\n",
        "            'total_epss_entries': len(epss_df),\n",
        "            'matched_stats': {\n",
        "                'found': found,\n",
        "                'not_found': not_found,\n",
        "                'not_found_examples': not_found_examples\n",
        "            }\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(INTERMEDIATE_DIR, 'matching_debug_info.json'), 'w') as f:\n",
        "            json.dump(debug_info, f, indent=2)\n",
        "\n",
        "        return matched\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore durante il matching: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def save_output(matched_list):\n",
        "    \"\"\"Salva i risultati\"\"\"\n",
        "    # Crea DataFrame\n",
        "    df = pd.DataFrame(matched_list, columns=[\"CVE_ID\", \"EPSS_Score\"])\n",
        "\n",
        "    # Aggiungi metadati\n",
        "    metadata = {\n",
        "        \"Data_Generazione\": get_current_time(),\n",
        "        \"Totale_CVE\": len(matched_list),\n",
        "        \"CVE_Con_EPSS\": len(df[df[\"EPSS_Score\"].notna()]),\n",
        "        \"CVE_Senza_EPSS\": len(df[df[\"EPSS_Score\"].isna()])\n",
        "    }\n",
        "\n",
        "    # Salva CSV principale\n",
        "    df.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    # Salva report\n",
        "    report_file = \"/content/cve_epss_report.txt\"\n",
        "    with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"=== Report Analisi CVE-EPSS ===\\n\\n\")\n",
        "        for key, value in metadata.items():\n",
        "            f.write(f\"{key}: {value}\\n\")\n",
        "\n",
        "        # Top 10 CVE per EPSS score\n",
        "        if len(df[df[\"EPSS_Score\"].notna()]) > 0:\n",
        "            f.write(\"\\nTop 10 CVE per EPSS Score:\\n\")\n",
        "            top_10 = df.nlargest(10, \"EPSS_Score\")\n",
        "            for _, row in top_10.iterrows():\n",
        "                f.write(f\"{row['CVE_ID']}: {row['EPSS_Score']:.4f}\\n\")\n",
        "\n",
        "    # Salva copie nella directory intermedia\n",
        "    ensure_directory(INTERMEDIATE_DIR)\n",
        "    df.to_csv(os.path.join(INTERMEDIATE_DIR, \"final_results.csv\"), index=False)\n",
        "    with open(os.path.join(INTERMEDIATE_DIR, \"final_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        with open(report_file, \"r\", encoding=\"utf-8\") as source:\n",
        "            f.write(source.read())\n",
        "\n",
        "    print(f\"[{get_current_time()}] ‚úÖ File salvati:\")\n",
        "    print(f\"   üìä CSV: {OUTPUT_FILE}\")\n",
        "    print(f\"   üìù Report: {report_file}\")\n",
        "    print(f\"   üìÅ File intermedi salvati in: {INTERMEDIATE_DIR}\")\n",
        "\n",
        "    # Download automatico\n",
        "    #files.download(OUTPUT_FILE)\n",
        "    #files.download(report_file)\n",
        "\n",
        "def main():\n",
        "    print(f\"[{get_current_time()}] üöÄ Avvio analisi CVE-EPSS\")\n",
        "\n",
        "    try:\n",
        "        # Crea directory per i file intermedi\n",
        "        ensure_directory(INTERMEDIATE_DIR)\n",
        "\n",
        "        # Download EPSS database\n",
        "        # download_epss_file()\n",
        "\n",
        "        # Carica CVE\n",
        "        cve_ids = load_cve_from_file()\n",
        "\n",
        "        if not cve_ids:\n",
        "            print(f\"[{get_current_time()}] ‚ö†Ô∏è Nessun CVE trovato nel file!\")\n",
        "            return\n",
        "\n",
        "        # Match con EPSS\n",
        "        matched = match_epss(cve_ids)\n",
        "\n",
        "        # Salva risultati\n",
        "        save_output(matched)\n",
        "\n",
        "        print(f\"[{get_current_time()}] ‚ú® Analisi completata con successo!\")\n",
        "        print(f\"[{get_current_time()}] üìÅ Tutti i file intermedi sono salvati in: {INTERMEDIATE_DIR}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[{get_current_time()}] ‚ùå Errore durante l'esecuzione: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as7lxpecfo_X",
        "outputId": "071396b0-cfce-4f28-bf66-16793a3bd487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-04-28 14:42:05] üöÄ Avvio analisi CVE-EPSS\n",
            "[2025-04-28 14:42:05] üìñ Leggendo CVE dal file...\n",
            "[2025-04-28 14:42:05] üîç Trovati 5404 CVE unici.\n",
            "[2025-04-28 14:42:05] üìä Caricamento database EPSS...\n",
            "\n",
            "DataFrame originale:\n",
            "              #model_version:v2025.03.14 score_date:2025-04-28T12:55:00Z\n",
            "cve                                 epss                      percentile\n",
            "CVE-1999-0001                    0.01297                         0.78576\n",
            "CVE-1999-0002                    0.16835                         0.94542\n",
            "CVE-1999-0003                    0.90483                         0.99563\n",
            "CVE-1999-0004                    0.04164                         0.88017\n",
            "\n",
            "Tipi di dati:\n",
            "#model_version:v2025.03.14         object\n",
            "score_date:2025-04-28T12:55:00Z    object\n",
            "dtype: object\n",
            "\n",
            "DataFrame dopo pulizia:\n",
            "             cve     epss\n",
            "1  CVE-1999-0001  0.01297\n",
            "2  CVE-1999-0002  0.16835\n",
            "3  CVE-1999-0003  0.90483\n",
            "4  CVE-1999-0004  0.04164\n",
            "5  CVE-1999-0005  0.17478\n",
            "\n",
            "[2025-04-28 14:44:33] üìà Statistiche matching:\n",
            "   ‚úÖ CVE trovati: 5404\n",
            "   ‚ùå CVE non trovati: 0\n",
            "[2025-04-28 14:44:33] ‚úÖ File salvati:\n",
            "   üìä CSV: /content/cve_with_epss.csv\n",
            "   üìù Report: /content/cve_epss_report.txt\n",
            "   üìÅ File intermedi salvati in: /content/intermediate_files\n",
            "[2025-04-28 14:44:33] ‚ú® Analisi completata con successo!\n",
            "[2025-04-28 14:44:33] üìÅ Tutti i file intermedi sono salvati in: /content/intermediate_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import re\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "class CVEImpactAnalyzer:\n",
        "    def __init__(self):\n",
        "        # Configurazione\n",
        "        self.BASE_DIR = \"/content/impact_analysis\"\n",
        "        self.DATA_DIR = f\"{self.BASE_DIR}/data\"\n",
        "        self.OUTPUT_DIR = f\"{self.BASE_DIR}/output\"\n",
        "\n",
        "        # File di input\n",
        "        self.CVE_FILE = \"/content/output_cve.txt\"\n",
        "        self.EPSS_FILE = \"/content/cve_with_epss.csv\"\n",
        "\n",
        "        # Pesi per il calcolo dell'impact score\n",
        "        self.weights = {\n",
        "            'epss_score': 0.6,           # EPSS ha un peso maggiore\n",
        "            'time_factor': 0.2,          # Fattore temporale\n",
        "            'prevalence_factor': 0.2     # Fattore di prevalenza\n",
        "        }\n",
        "\n",
        "        # Timestamp corrente\n",
        "        self.current_time = datetime.utcnow()\n",
        "\n",
        "        # Crea le directory necessarie\n",
        "        self._create_directories()\n",
        "\n",
        "    def _create_directories(self):\n",
        "        \"\"\"Crea le directory necessarie per l'analisi\"\"\"\n",
        "        for directory in [self.BASE_DIR, self.DATA_DIR, self.OUTPUT_DIR]:\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "                print(f\"Created directory: {directory}\")\n",
        "\n",
        "    def load_data(self) -> Tuple[List[str], pd.DataFrame]:\n",
        "        \"\"\"Carica i dati dai file esistenti\"\"\"\n",
        "        # Carica lista CVE\n",
        "        with open(self.CVE_FILE, 'r') as f:\n",
        "            content = f.read()\n",
        "            cve_list = re.findall(r'CVE-\\d{4}-\\d+', content)\n",
        "            cve_list = list(set(cve_list))  # Rimuovi duplicati\n",
        "\n",
        "        # Carica dati EPSS\n",
        "        epss_df = pd.read_csv(self.EPSS_FILE)\n",
        "\n",
        "        print(f\"Loaded {len(cve_list)} unique CVEs\")\n",
        "        print(f\"Loaded EPSS data with shape {epss_df.shape}\")\n",
        "\n",
        "        return cve_list, epss_df\n",
        "\n",
        "    def calculate_time_factor(self, cve_id: str) -> float:\n",
        "        \"\"\"Calcola il fattore temporale basato sull'anno del CVE\"\"\"\n",
        "        try:\n",
        "            year = int(cve_id.split('-')[1])\n",
        "            current_year = self.current_time.year\n",
        "\n",
        "            # Calcola il decadimento temporale\n",
        "            age = current_year - year\n",
        "            time_factor = np.exp(-age / 5)  # Decadimento esponenziale su 5 anni\n",
        "\n",
        "            return time_factor\n",
        "        except:\n",
        "            return 0.5  # Valore di default se c'√® un errore\n",
        "\n",
        "    def calculate_prevalence_factor(self, cve_id: str, all_cves: List[str]) -> float:\n",
        "        \"\"\"Calcola un fattore di prevalenza basato sull'anno del CVE\"\"\"\n",
        "        try:\n",
        "            year = cve_id.split('-')[1]\n",
        "            same_year_cves = len([cve for cve in all_cves if year in cve])\n",
        "            total_cves = len(all_cves)\n",
        "\n",
        "            # Normalizza il fattore di prevalenza\n",
        "            prevalence = same_year_cves / total_cves if total_cves > 0 else 0\n",
        "            return 1 - prevalence  # Inverti il fattore (pi√π raro = pi√π importante)\n",
        "        except:\n",
        "            return 0.5\n",
        "\n",
        "    def calculate_impact_scores(self, cve_list: List[str], epss_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Calcola gli impact score per tutti i CVE\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for cve_id in cve_list:\n",
        "            # Trova EPSS score\n",
        "            epss_score = epss_df[epss_df['CVE_ID'] == cve_id]['EPSS_Score'].iloc[0] if len(epss_df[epss_df['CVE_ID'] == cve_id]) > 0 else None\n",
        "\n",
        "            # Calcola altri fattori\n",
        "            time_factor = self.calculate_time_factor(cve_id)\n",
        "            prevalence_factor = self.calculate_prevalence_factor(cve_id, cve_list)\n",
        "\n",
        "            # Calcola impact score\n",
        "            if epss_score is not None:\n",
        "                total_score = (\n",
        "                    self.weights['epss_score'] * epss_score +\n",
        "                    self.weights['time_factor'] * time_factor +\n",
        "                    self.weights['prevalence_factor'] * prevalence_factor\n",
        "                )\n",
        "            else:\n",
        "                total_score = None\n",
        "\n",
        "            # Prepara il risultato\n",
        "            result = {\n",
        "                'cve_id': cve_id,\n",
        "                'epss_score': epss_score,\n",
        "                'time_factor': time_factor,\n",
        "                'prevalence_factor': prevalence_factor,\n",
        "                'total_impact_score': total_score,\n",
        "                'year': int(cve_id.split('-')[1]),\n",
        "                'has_epss': epss_score is not None\n",
        "            }\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def analyze_and_save(self) -> pd.DataFrame:\n",
        "        \"\"\"Esegue l'analisi completa e salva i risultati\"\"\"\n",
        "        # Carica i dati\n",
        "        cve_list, epss_df = self.load_data()\n",
        "\n",
        "        # Calcola gli impact score\n",
        "        results_df = self.calculate_impact_scores(cve_list, epss_df)\n",
        "\n",
        "        # Aggiungi statistiche per anno\n",
        "        results_df['year_avg_impact'] = results_df.groupby('year')['total_impact_score'].transform('mean')\n",
        "        results_df['year_count'] = results_df.groupby('year')['cve_id'].transform('count')\n",
        "\n",
        "        # Salva i risultati\n",
        "        timestamp = self.current_time.strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "        # Salva il dataset completo\n",
        "        output_file = f\"{self.OUTPUT_DIR}/impact_analysis_{timestamp}.csv\"\n",
        "        results_df.to_csv(output_file, index=False)\n",
        "\n",
        "        # Crea e salva il report\n",
        "        report_file = f\"{self.OUTPUT_DIR}/analysis_report_{timestamp}.txt\"\n",
        "        self.create_report(results_df, report_file)\n",
        "\n",
        "        # Prepara il dataset per machine learning\n",
        "        ml_ready_df = self.prepare_ml_dataset(results_df)\n",
        "        ml_file = f\"{self.OUTPUT_DIR}/ml_ready_dataset.csv\"\n",
        "        ml_ready_df.to_csv(ml_file, index=False)\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def create_report(self, df: pd.DataFrame, output_file: str):\n",
        "        \"\"\"Crea un report dettagliato dell'analisi\"\"\"\n",
        "        with open(output_file, 'w') as f:\n",
        "            f.write(f\"CVE Impact Analysis Report\\n\")\n",
        "            f.write(f\"Generated on: {self.current_time.isoformat()}\\n\\n\")\n",
        "\n",
        "            f.write(\"General Statistics:\\n\")\n",
        "            f.write(f\"Total CVEs analyzed: {len(df)}\\n\")\n",
        "            f.write(f\"CVEs with EPSS scores: {df['has_epss'].sum()}\\n\")\n",
        "            f.write(f\"Average impact score: {df['total_impact_score'].mean():.4f}\\n\\n\")\n",
        "\n",
        "            f.write(\"Impact Score Distribution:\\n\")\n",
        "            f.write(df['total_impact_score'].describe().to_string())\n",
        "            f.write(\"\\n\\n\")\n",
        "\n",
        "            f.write(\"Top 10 Highest Impact CVEs:\\n\")\n",
        "            top_10 = df.nlargest(10, 'total_impact_score')\n",
        "            for _, row in top_10.iterrows():\n",
        "                f.write(f\"{row['cve_id']}: {row['total_impact_score']:.4f}\\n\")\n",
        "\n",
        "            f.write(\"\\nYearly Statistics:\\n\")\n",
        "            yearly_stats = df.groupby('year').agg({\n",
        "                'total_impact_score': ['count', 'mean', 'std'],\n",
        "                'epss_score': 'mean'\n",
        "            }).round(4)\n",
        "            f.write(yearly_stats.to_string())\n",
        "\n",
        "    def prepare_ml_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Prepara il dataset per machine learning\"\"\"\n",
        "        ml_df = df.copy()\n",
        "\n",
        "        # Aggiungi feature derivate\n",
        "        ml_df['year_normalized'] = (ml_df['year'] - ml_df['year'].min()) / (ml_df['year'].max() - ml_df['year'].min())\n",
        "        ml_df['has_epss_binary'] = ml_df['has_epss'].astype(int)\n",
        "        ml_df['impact_score_binned'] = pd.qcut(ml_df['total_impact_score'].fillna(0), q=5, labels=['VL', 'L', 'M', 'H', 'VH'])\n",
        "\n",
        "        # Calcola statistiche rolling per anno\n",
        "        ml_df = ml_df.sort_values('year')\n",
        "        ml_df['rolling_avg_impact'] = ml_df.groupby('year')['total_impact_score'].transform(\n",
        "            lambda x: x.rolling(window=3, min_periods=1).mean()\n",
        "        )\n",
        "\n",
        "        return ml_df\n",
        "\n",
        "def main():\n",
        "    # Inizializza l'analizzatore\n",
        "    analyzer = CVEImpactAnalyzer()\n",
        "\n",
        "    # Esegui l'analisi\n",
        "    print(\"Starting CVE impact analysis...\")\n",
        "    results_df = analyzer.analyze_and_save()\n",
        "\n",
        "    # Stampa alcune statistiche\n",
        "    print(\"\\nAnalysis completed!\")\n",
        "    print(f\"Total CVEs analyzed: {len(results_df)}\")\n",
        "    print(f\"CVEs with impact scores: {results_df['total_impact_score'].notna().sum()}\")\n",
        "    print(\"\\nImpact Score Statistics:\")\n",
        "    print(results_df['total_impact_score'].describe())\n",
        "\n",
        "    # Download automatico dei file\n",
        "    #files.download(f\"{analyzer.OUTPUT_DIR}/impact_analysis_{analyzer.current_time.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "    #files.download(f\"{analyzer.OUTPUT_DIR}/analysis_report_{analyzer.current_time.strftime('%Y%m%d_%H%M%S')}.txt\")\n",
        "    #files.download(f\"{analyzer.OUTPUT_DIR}/ml_ready_dataset_{analyzer.current_time.strftime('%Y%m%d_%H%M%S')}.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxgdnzvBmGlb",
        "outputId": "701bb725-35dc-4190-c1c0-3de3b80ccc07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting CVE impact analysis...\n",
            "Loaded 5404 unique CVEs\n",
            "Loaded EPSS data with shape (5404, 2)\n",
            "\n",
            "Analysis completed!\n",
            "Total CVEs analyzed: 5404\n",
            "CVEs with impact scores: 5404\n",
            "\n",
            "Impact Score Statistics:\n",
            "count    5404.000000\n",
            "mean        0.298035\n",
            "std         0.094373\n",
            "min         0.235514\n",
            "25%         0.238942\n",
            "50%         0.269679\n",
            "75%         0.314736\n",
            "max         0.947940\n",
            "Name: total_impact_score, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy scikit-learn seaborn matplotlib joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnu5E7oltl8H",
        "outputId": "9332e80a-2211-4a32-897e-a6c450192da1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import joblib\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "class ThreatRiskRegressor:\n",
        "    def __init__(self):\n",
        "        # Configurazione\n",
        "        self.BASE_DIR = \"/content/risk_assessment\"\n",
        "        self.MODELS_DIR = f\"{self.BASE_DIR}/models\"\n",
        "        self.RESULTS_DIR = f\"{self.BASE_DIR}/results\"\n",
        "        self.DATA_DIR = f\"{self.BASE_DIR}/data\"\n",
        "\n",
        "        # File di input precedenti\n",
        "        self.CVE_EPSS_FILE = \"/content/cve_with_epss.csv\"\n",
        "        self.IMPACT_FILE = \"/content/impact_analysis/output/ml_ready_dataset.csv\"\n",
        "\n",
        "        # Configurazione delle colonne\n",
        "        self.numeric_features = ['asset_value', 'affected_users', 'business_impact', 'existing_controls']\n",
        "        self.categorical_features = ['attack_vector', 'authentication_required']\n",
        "\n",
        "        # Inizializzazione\n",
        "        self.model = None\n",
        "        self.preprocessor = None\n",
        "        self._create_directories()\n",
        "\n",
        "    def _create_directories(self):\n",
        "        \"\"\"Crea le directory necessarie\"\"\"\n",
        "        for directory in [self.BASE_DIR, self.MODELS_DIR, self.RESULTS_DIR, self.DATA_DIR]:\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "                print(f\"Created directory: {directory}\")\n",
        "\n",
        "    def create_sample_input(self):\n",
        "        \"\"\"Crea un file di input di esempio\"\"\"\n",
        "        sample_data = {\n",
        "            'threat_name': ['DataBreach2025'],\n",
        "            'asset_value': [8],\n",
        "            'affected_users': [5000],\n",
        "            'business_impact': [9],\n",
        "            'attack_vector': ['network'],\n",
        "            'authentication_required': [False],\n",
        "            'existing_controls': [2]\n",
        "        }\n",
        "\n",
        "        df = pd.DataFrame(sample_data)\n",
        "        sample_file = f\"{self.DATA_DIR}/sample_threat_input.csv\"\n",
        "        df.to_csv(sample_file, index=False)\n",
        "        print(f\"Sample input file created: {sample_file}\")\n",
        "        return sample_file\n",
        "\n",
        "    def prepare_historical_data(self):\n",
        "        \"\"\"Prepara i dati storici dai file precedenti\"\"\"\n",
        "        # Carica i dati EPSS e Impact\n",
        "        epss_df = pd.read_csv(self.CVE_EPSS_FILE)\n",
        "        impact_df = pd.read_csv(self.IMPACT_FILE)\n",
        "\n",
        "        # Unisci i dataset\n",
        "        historical_data = pd.merge(epss_df, impact_df, left_on='CVE_ID', right_on='cve_id', how='inner')\n",
        "\n",
        "        # Crea feature sintetiche per il training\n",
        "        historical_data['asset_value'] = np.random.randint(1, 11, size=len(historical_data))\n",
        "        historical_data['affected_users'] = np.random.randint(100, 10000, size=len(historical_data))\n",
        "        historical_data['business_impact'] = historical_data['total_impact_score'] * 10\n",
        "        historical_data['attack_vector'] = np.random.choice(['network', 'local', 'physical', 'adjacent'], size=len(historical_data))\n",
        "        historical_data['authentication_required'] = np.random.choice([True, False], size=len(historical_data))\n",
        "        historical_data['existing_controls'] = np.random.randint(0, 5, size=len(historical_data))\n",
        "\n",
        "        return historical_data\n",
        "\n",
        "    def create_preprocessor(self):\n",
        "        \"\"\"Crea il preprocessor per le feature\"\"\"\n",
        "        numeric_transformer = StandardScaler()\n",
        "        categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)\n",
        "\n",
        "        self.preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, self.numeric_features),\n",
        "                ('cat', categorical_transformer, self.categorical_features)\n",
        "            ])\n",
        "\n",
        "        return self.preprocessor\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"Addestra il modello usando i dati storici\"\"\"\n",
        "        try:\n",
        "            # Prepara i dati\n",
        "            historical_data = self.prepare_historical_data()\n",
        "\n",
        "            # Prepara X e y\n",
        "            X = historical_data[self.numeric_features + self.categorical_features]\n",
        "            y = historical_data['total_impact_score']\n",
        "\n",
        "            # Crea pipeline\n",
        "            self.model = Pipeline([\n",
        "                ('preprocessor', self.create_preprocessor()),\n",
        "                ('regressor', ElasticNet(random_state=42))\n",
        "            ])\n",
        "\n",
        "            # Parametri per GridSearchCV\n",
        "            param_grid = {\n",
        "                'regressor__alpha': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
        "                'regressor__l1_ratio': [0.1, 0.5, 0.9],\n",
        "            }\n",
        "\n",
        "            # GridSearchCV\n",
        "            grid_search = GridSearchCV(\n",
        "                self.model,\n",
        "                param_grid,\n",
        "                cv=5,\n",
        "                scoring='neg_mean_squared_error',\n",
        "                n_jobs=-1\n",
        "            )\n",
        "\n",
        "            # Fit\n",
        "            grid_search.fit(X, y)\n",
        "            self.model = grid_search.best_estimator_\n",
        "\n",
        "            return grid_search.best_params_\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore durante il training del modello: {str(e)}\")\n",
        "            print(\"\\nDati di debug:\")\n",
        "            print(\"Shape of X:\", X.shape if 'X' in locals() else \"X not created\")\n",
        "            print(\"Shape of y:\", y.shape if 'y' in locals() else \"y not created\")\n",
        "            print(\"Columns in historical_data:\", historical_data.columns if 'historical_data' in locals() else \"historical_data not created\")\n",
        "            raise\n",
        "\n",
        "    def predict_risk(self, input_file: str) -> pd.DataFrame:\n",
        "        \"\"\"Predici il risk score per nuovi dati\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained!\")\n",
        "\n",
        "        try:\n",
        "            # Carica input\n",
        "            input_df = pd.read_csv(input_file)\n",
        "\n",
        "            # Verifica colonne\n",
        "            required_columns = self.numeric_features + self.categorical_features + ['threat_name']\n",
        "            missing_columns = set(required_columns) - set(input_df.columns)\n",
        "            if missing_columns:\n",
        "                raise ValueError(f\"Missing columns in input file: {missing_columns}\")\n",
        "\n",
        "            # Predici\n",
        "            X = input_df[self.numeric_features + self.categorical_features]\n",
        "            risk_scores = self.model.predict(X)\n",
        "\n",
        "            # Prepara risultati\n",
        "            results = input_df[['threat_name']].copy()\n",
        "            results['risk_score'] = risk_scores\n",
        "            results['risk_level'] = pd.qcut(risk_scores, q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "            # Aggiungi dettagli addizionali\n",
        "            results['timestamp'] = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            results['input_features'] = X.apply(lambda row: dict(row), axis=1)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore durante la predizione: {str(e)}\")\n",
        "            print(\"\\nDati di debug:\")\n",
        "            print(\"Input file columns:\", input_df.columns if 'input_df' in locals() else \"input_df not loaded\")\n",
        "            print(\"Required columns:\", required_columns)\n",
        "            raise\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Inizializza\n",
        "        risk_regressor = ThreatRiskRegressor()\n",
        "\n",
        "        # Crea file di esempio se necessario\n",
        "        sample_file = risk_regressor.create_sample_input()\n",
        "        print(\"\\nEsempio di file di input creato. Struttura:\")\n",
        "        print(pd.read_csv(sample_file).to_string())\n",
        "\n",
        "        # Addestra il modello\n",
        "        print(\"\\nAddestrando il modello...\")\n",
        "        best_params = risk_regressor.train_model()\n",
        "        print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "        # Carica il file di input custom\n",
        "        print(\"\\nPer favore, carica il tuo file CSV di input (o usa il file di esempio)...\")\n",
        "        try:\n",
        "            uploaded = files.upload()\n",
        "            input_file = next(iter(uploaded))\n",
        "        except:\n",
        "            print(\"Usando il file di esempio...\")\n",
        "            input_file = sample_file\n",
        "\n",
        "        # Predici il rischio\n",
        "        results = risk_regressor.predict_risk(input_file)\n",
        "\n",
        "        # Stampa e salva risultati\n",
        "        print(\"\\nRisultati dell'analisi del rischio:\")\n",
        "        print(results.to_string())\n",
        "\n",
        "        # Salva risultati\n",
        "        timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "        output_file = f\"{risk_regressor.RESULTS_DIR}/risk_assessment_{timestamp}.csv\"\n",
        "        results.to_csv(output_file, index=False)\n",
        "        print(f\"\\nRisultati salvati in: {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore nell'esecuzione del programma: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "j1FFcxJ_vWRL",
        "outputId": "41429267-cfa9-4e20-dc2d-98f50221c6a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input file created: /content/risk_assessment/data/sample_threat_input.csv\n",
            "\n",
            "Esempio di file di input creato. Struttura:\n",
            "      threat_name  asset_value  affected_users  business_impact attack_vector  authentication_required  existing_controls\n",
            "0  DataBreach2025            8            5000                9       network                    False                  2\n",
            "\n",
            "Addestrando il modello...\n",
            "Best parameters: {'regressor__alpha': 0.0001, 'regressor__l1_ratio': 0.1}\n",
            "\n",
            "Per favore, carica il tuo file CSV di input (o usa il file di esempio)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a4cbfbf2-fc60-48b0-9993-428e83a2f1cb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a4cbfbf2-fc60-48b0-9993-428e83a2f1cb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving custom_threat_input.csv to custom_threat_input (3).csv\n",
            "\n",
            "Risultati dell'analisi del rischio:\n",
            "        threat_name  risk_score risk_level            timestamp                                                                                                                                           input_features\n",
            "0    DataBreach2025    0.899882       High  2025-04-28 14:45:09   {'asset_value': 8, 'affected_users': 5000, 'business_impact': 9, 'existing_controls': 2, 'attack_vector': 'network', 'authentication_required': False}\n",
            "1  RansomwareAttack    0.999862  Very High  2025-04-28 14:45:09  {'asset_value': 10, 'affected_users': 3000, 'business_impact': 10, 'existing_controls': 3, 'attack_vector': 'network', 'authentication_required': True}\n",
            "2  PhishingCampaign    0.699921   Very Low  2025-04-28 14:45:09   {'asset_value': 6, 'affected_users': 1000, 'business_impact': 7, 'existing_controls': 4, 'attack_vector': 'network', 'authentication_required': False}\n",
            "3    InternalThreat    0.799902        Low  2025-04-28 14:45:09       {'asset_value': 7, 'affected_users': 200, 'business_impact': 8, 'existing_controls': 5, 'attack_vector': 'local', 'authentication_required': True}\n",
            "4        DDoSAttack    0.799902        Low  2025-04-28 14:45:09   {'asset_value': 9, 'affected_users': 8000, 'business_impact': 8, 'existing_controls': 3, 'attack_vector': 'network', 'authentication_required': False}\n",
            "\n",
            "Risultati salvati in: /content/risk_assessment/results/risk_assessment_20250428_144509.csv\n"
          ]
        }
      ]
    }
  ]
}